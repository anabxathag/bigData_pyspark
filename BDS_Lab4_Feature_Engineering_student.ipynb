{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1N4B9KRtLAQgnH9sAB2MJpcpi6SR6HH4C","timestamp":1738036359627},{"file_id":"1thygjITi5z_j6PYx0ea6TDN0vsJHnjpE","timestamp":1738030736365}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Lab 4: Machine Learning Library (MLlib)**\n","\n","Lecturer: `Sirasit Lochanachit`\n","\n","\n","Course: `06026213 Big Data Systems`\n","\n","Term: `02/2024`\n","\n","Lab materials prepared by `Duangkamon Phobsungnoen (TA)`"],"metadata":{"id":"NKrlYhdDWlxv"}},{"cell_type":"markdown","source":["# **Example 1: Estimator, Transformer, and Param**"],"metadata":{"id":"z7qCk24LWtDe"}},{"cell_type":"markdown","source":["**STEP 0 : ติดตั้ง PySpark**\n","\n"],"metadata":{"id":"hbdqtOSFFXTG"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"8VdgJgp5l-zz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738036756878,"user_tz":-420,"elapsed":12222,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}},"outputId":"459bad56-d04e-4df1-b8b1-371ca795b82e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.11/dist-packages (3.5.4)\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.11/dist-packages (from pyspark) (0.10.9.7)\n"]}],"source":["!pip install pyspark"]},{"cell_type":"markdown","source":["**STEP 1 : สร้าง SparkSession**"],"metadata":{"id":"auMZoACOHNdN"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.master(\"local\").appName(\"PySparkExample\").getOrCreate()"],"metadata":{"id":"qQSI1jbjH7xG","executionInfo":{"status":"ok","timestamp":1738038439422,"user_tz":-420,"elapsed":366,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}}},"execution_count":34,"outputs":[]},{"cell_type":"markdown","source":["ใช้บน local ของ colab & ใช้แค่ 1 thread ใน cpu = ไม่ได้เกิดการประมวลผลแบบ parallel"],"metadata":{"id":"dcVZqFF328Gr"}},{"cell_type":"markdown","source":["**STEP 2 : สร้าง DataFrame**"],"metadata":{"id":"6OsrnKgZFwBP"}},{"cell_type":"markdown","source":["สร้าง DataFrame สำหรับฝึกโมเดลที่ประกอบด้วย list ของ (label, features) tuples\n","\n","**Feature Vectors**\n","- ข้อมูลที่ใช้ train ของ MLlib ควรอยู่ในรูปของ feature vectors\n","- ข้อมูลของแต่ละแถวจะอยู่ใน vector (list) เดียวกัน โดยสมาชิกของ vector คือค่าของแต่ละ column\n","\n","\n","**Label index**\n","- Column ที่เป็น label ต้องมีค่าเป็น 0/1 ไม่สามารถเป็น string เช่น Yes/No ได้\n"],"metadata":{"id":"lNqAnR3cG40J"}},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors\n","\n","#ตัวแรก คือ target label, ต่อมาคือ vector, each row in the same list\n","training = spark.createDataFrame([\n","    (1.0, Vectors.dense([0.0, 1.1, 0.1])),\n","    (0.0, Vectors.dense([2.0, 1.0, -1.0])),\n","    (0.0, Vectors.dense([2.0, 1.3, 1.0])),\n","    (1.0, Vectors.dense([0.0, 1.2, -0.5]))], [\"label\", \"features\"])\n","# df นี้มี 2 col, 1 vector = 1 col แล้ว"],"metadata":{"id":"t3i7OSvYGAbn","executionInfo":{"status":"ok","timestamp":1738038442218,"user_tz":-420,"elapsed":540,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}}},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":["**เพิ่มเติม:** Vectors.dense คือวิธีการสร้างเวกเตอร์ที่มีค่าคงที่ (dense vector) ซึ่งถูกใช้เพื่อแทนข้อมูลในรูปแบบของเวกเตอร์ในลักษณะทางคณิตศาสตร์ เช่น การเก็บข้อมูลหลายมิติที่มีค่าเป็นตัวเลขในรูปแบบของเวกเตอร์\n","\n","ตัวอย่าง\n","\n","`Vectors.dense([0.0, 1.1, 0.1])`\n","\n","จะสร้างเวกเตอร์ที่มีสามค่า คือ 0.0, 1.1, และ 0.1 ซึ่งแต่ละค่าจะเป็นข้อมูลที่ใช้แทนคุณสมบัติต่างๆ ในการเรียนรู้ของโมเดล machine learning เช่น classification หรือ regression"],"metadata":{"id":"XxgXZJ2BtnWt"}},{"cell_type":"markdown","source":["**STEP 3 : สร้าง Logistic Regression Estimator**"],"metadata":{"id":"eaxV_NqnLkhD"}},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n","lr = LogisticRegression(maxIter=10)"],"metadata":{"id":"7lphHdQ3No3c","executionInfo":{"status":"ok","timestamp":1738038449858,"user_tz":-420,"elapsed":450,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}}},"execution_count":36,"outputs":[]},{"cell_type":"markdown","source":["แสดง Paramater ทั้งหมด รวมถึงคำอธิบายประกอบและค่า default"],"metadata":{"id":"7aSdLTarXOFk"}},{"cell_type":"code","source":["print(\"LogisticRegression parameters:\\n\" + lr.explainParams() + \"\\n\")"],"metadata":{"id":"6uIHuSCHXNnC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738038450236,"user_tz":-420,"elapsed":9,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}},"outputId":"094d0f80-d27f-45f6-8fa6-20426fa9bcfb"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["LogisticRegression parameters:\n","aggregationDepth: suggested depth for treeAggregate (>= 2). (default: 2)\n","elasticNetParam: the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty. (default: 0.0)\n","family: The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial (default: auto)\n","featuresCol: features column name. (default: features)\n","fitIntercept: whether to fit an intercept term. (default: True)\n","labelCol: label column name. (default: label)\n","lowerBoundsOnCoefficients: The lower bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n","lowerBoundsOnIntercepts: The lower bounds on intercepts if fitting under bound constrained optimization. The bounds vector size must beequal with 1 for binomial regression, or the number oflasses for multinomial regression. (undefined)\n","maxBlockSizeInMB: maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0. (default: 0.0)\n","maxIter: max number of iterations (>= 0). (default: 100, current: 10)\n","predictionCol: prediction column name. (default: prediction)\n","probabilityCol: Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities. (default: probability)\n","rawPredictionCol: raw prediction (a.k.a. confidence) column name. (default: rawPrediction)\n","regParam: regularization parameter (>= 0). (default: 0.0)\n","standardization: whether to standardize the training features before fitting the model. (default: True)\n","threshold: Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p]. (default: 0.5)\n","thresholds: Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. (undefined)\n","tol: the convergence tolerance for iterative algorithms (>= 0). (default: 1e-06)\n","upperBoundsOnCoefficients: The upper bounds on coefficients if fitting under bound constrained optimization. The bound matrix must be compatible with the shape (1, number of features) for binomial regression, or (number of classes, number of features) for multinomial regression. (undefined)\n","upperBoundsOnIntercepts: The upper bounds on intercepts if fitting under bound constrained optimization. The bound vector size must be equal with 1 for binomial regression, or the number of classes for multinomial regression. (undefined)\n","weightCol: weight column name. If this is not set or empty, we treat all instance weights as 1.0. (undefined)\n","\n"]}]},{"cell_type":"markdown","source":["**STEP 4 : ฝึกโมเดลและดู parameter**"],"metadata":{"id":"lfOt1faCNomI"}},{"cell_type":"code","source":["model1 = lr.fit(training)"],"metadata":{"id":"Wzb35vZVOcsC","executionInfo":{"status":"ok","timestamp":1738038453254,"user_tz":-420,"elapsed":2431,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["print(\"Model 1 was fit using parameters: \")\n","print(model1.extractParamMap())"],"metadata":{"id":"E2dfgcUVXkYN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738038466433,"user_tz":-420,"elapsed":398,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}},"outputId":"8edaae75-12e4-4545-c177-b7f6c1d1e7bc"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Model 1 was fit using parameters: \n","{Param(parent='LogisticRegression_5d383996d31c', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2, Param(parent='LogisticRegression_5d383996d31c', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0, Param(parent='LogisticRegression_5d383996d31c', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto', Param(parent='LogisticRegression_5d383996d31c', name='featuresCol', doc='features column name.'): 'features', Param(parent='LogisticRegression_5d383996d31c', name='fitIntercept', doc='whether to fit an intercept term.'): True, Param(parent='LogisticRegression_5d383996d31c', name='labelCol', doc='label column name.'): 'label', Param(parent='LogisticRegression_5d383996d31c', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0, Param(parent='LogisticRegression_5d383996d31c', name='maxIter', doc='max number of iterations (>= 0).'): 10, Param(parent='LogisticRegression_5d383996d31c', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='LogisticRegression_5d383996d31c', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability', Param(parent='LogisticRegression_5d383996d31c', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction', Param(parent='LogisticRegression_5d383996d31c', name='regParam', doc='regularization parameter (>= 0).'): 0.0, Param(parent='LogisticRegression_5d383996d31c', name='standardization', doc='whether to standardize the training features before fitting the model.'): True, Param(parent='LogisticRegression_5d383996d31c', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.5, Param(parent='LogisticRegression_5d383996d31c', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}\n"]}]},{"cell_type":"markdown","source":["**STEP 5 : ปรับแต่ง parameter ด้วย ParamMap**"],"metadata":{"id":"1rbbr_ajOh-b"}},{"cell_type":"markdown","source":["สามารถเปลี่ยนแปลงพารามิเตอร์ต่างๆ เช่น maxIter, regParam เป็นต้น และใช้ ParamMap ในการกำหนด parameter"],"metadata":{"id":"rXt1Cl2ROqdl"}},{"cell_type":"code","source":["# สร้าง param map เป็น dict ของ parameter config\n","paramMap = {lr.maxIter: 20, lr.regParam: 0.1}\n","\n","# update param\n","paramMap[lr.maxIter] = 30 # method 1 : index\n","paramMap.update({lr.threshold: 0.55}) # method 2 : update\n","\n","# paramMap\n","model2 = lr.fit(training, paramMap)"],"metadata":{"id":"jYFgMOaGOlhQ","executionInfo":{"status":"ok","timestamp":1738038474478,"user_tz":-420,"elapsed":2001,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}}},"execution_count":40,"outputs":[]},{"cell_type":"code","source":["print(\"Model 2 was fit using parameters: \")\n","print(model2.extractParamMap())"],"metadata":{"id":"qFJCh1xoXnNQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738038474479,"user_tz":-420,"elapsed":3,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}},"outputId":"4a6eb46a-6278-439d-c57f-13dc70b217eb"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Model 2 was fit using parameters: \n","{Param(parent='LogisticRegression_5d383996d31c', name='aggregationDepth', doc='suggested depth for treeAggregate (>= 2).'): 2, Param(parent='LogisticRegression_5d383996d31c', name='elasticNetParam', doc='the ElasticNet mixing parameter, in range [0, 1]. For alpha = 0, the penalty is an L2 penalty. For alpha = 1, it is an L1 penalty.'): 0.0, Param(parent='LogisticRegression_5d383996d31c', name='family', doc='The name of family which is a description of the label distribution to be used in the model. Supported options: auto, binomial, multinomial'): 'auto', Param(parent='LogisticRegression_5d383996d31c', name='featuresCol', doc='features column name.'): 'features', Param(parent='LogisticRegression_5d383996d31c', name='fitIntercept', doc='whether to fit an intercept term.'): True, Param(parent='LogisticRegression_5d383996d31c', name='labelCol', doc='label column name.'): 'label', Param(parent='LogisticRegression_5d383996d31c', name='maxBlockSizeInMB', doc='maximum memory in MB for stacking input data into blocks. Data is stacked within partitions. If more than remaining data size in a partition then it is adjusted to the data size. Default 0.0 represents choosing optimal value, depends on specific algorithm. Must be >= 0.'): 0.0, Param(parent='LogisticRegression_5d383996d31c', name='maxIter', doc='max number of iterations (>= 0).'): 30, Param(parent='LogisticRegression_5d383996d31c', name='predictionCol', doc='prediction column name.'): 'prediction', Param(parent='LogisticRegression_5d383996d31c', name='probabilityCol', doc='Column name for predicted class conditional probabilities. Note: Not all models output well-calibrated probability estimates! These probabilities should be treated as confidences, not precise probabilities.'): 'probability', Param(parent='LogisticRegression_5d383996d31c', name='rawPredictionCol', doc='raw prediction (a.k.a. confidence) column name.'): 'rawPrediction', Param(parent='LogisticRegression_5d383996d31c', name='regParam', doc='regularization parameter (>= 0).'): 0.1, Param(parent='LogisticRegression_5d383996d31c', name='standardization', doc='whether to standardize the training features before fitting the model.'): True, Param(parent='LogisticRegression_5d383996d31c', name='threshold', doc='Threshold in binary classification prediction, in range [0, 1]. If threshold and thresholds are both set, they must match.e.g. if threshold is p, then thresholds must be equal to [1-p, p].'): 0.55, Param(parent='LogisticRegression_5d383996d31c', name='tol', doc='the convergence tolerance for iterative algorithms (>= 0).'): 1e-06}\n"]}]},{"cell_type":"markdown","source":["**STEP 6 : สร้างข้อมูลทดสอบ (test data)**"],"metadata":{"id":"thS1jfqEPCeO"}},{"cell_type":"code","source":["test = spark.createDataFrame([\n","    (1.0, Vectors.dense([-1.0, 1.5, 1.3])),\n","    (0.0, Vectors.dense([3.0, 2.0, -0.1])),\n","    (1.0, Vectors.dense([0.0, 2.2, -1.5]))], [\"label\", \"features\"])"],"metadata":{"id":"F-yr0xIfPH6B","executionInfo":{"status":"ok","timestamp":1738038475896,"user_tz":-420,"elapsed":5,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}}},"execution_count":42,"outputs":[]},{"cell_type":"markdown","source":["**STEP 7 : ทำนายผลจากโมเดล**"],"metadata":{"id":"OgbSPeQXPNGf"}},{"cell_type":"markdown","source":["ใช้ Transformer เพื่อทำการทำนายผลจากข้อมูลทดสอบ"],"metadata":{"id":"DEvy3BW1PWGu"}},{"cell_type":"code","source":["prediction = model2.transform(test) # แปลง test df ให้เป็นอีก df นึง\n","prediction"],"metadata":{"id":"OFzwkIIxPQdC","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738038478086,"user_tz":-420,"elapsed":615,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}},"outputId":"bde5feff-3860-46d3-b7e3-b3c05c32b8cf"},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataFrame[label: double, features: vector, rawPrediction: vector, probability: vector, prediction: double]"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["result = prediction.select('features', 'label', 'probability', 'prediction').collect()\n","for row in result:\n","  print('feature = %s, label = %s -> prob = %s, prediction = %s' % (row.features, row.label, row.probability, row.prediction))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t2wYwPjb8Ugy","executionInfo":{"status":"ok","timestamp":1738038478711,"user_tz":-420,"elapsed":629,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}},"outputId":"56018432-0bd8-496d-f02b-8f8e2fe24671"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["feature = [-1.0,1.5,1.3], label = 1.0 -> prob = [0.05707304993572542,0.9429269500642746], prediction = 1.0\n","feature = [3.0,2.0,-0.1], label = 0.0 -> prob = [0.9238521956443227,0.07614780435567725], prediction = 0.0\n","feature = [0.0,2.2,-1.5], label = 1.0 -> prob = [0.10972780286187782,0.8902721971381222], prediction = 1.0\n"]}]},{"cell_type":"code","source":["model_test_result = model2.evaluate(test)\n","\n","model_test_result"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yoXRbSKWKahz","executionInfo":{"status":"ok","timestamp":1738038575424,"user_tz":-420,"elapsed":396,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}},"outputId":"ae1d2325-2233-4fb6-acad-2426d6b19ac3"},"execution_count":45,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<pyspark.ml.classification.BinaryLogisticRegressionSummary at 0x7d74ff7a9d90>"]},"metadata":{},"execution_count":45}]},{"cell_type":"code","source":["model_test_result.accuracy, model_test_result.recallByLabel"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Eu1nASK6LBBo","executionInfo":{"status":"ok","timestamp":1738038655329,"user_tz":-420,"elapsed":415,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}},"outputId":"df8bd6b2-6eac-4b78-988b-1a4ae8f78dfa"},"execution_count":48,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1.0, [1.0, 1.0])"]},"metadata":{},"execution_count":48}]},{"cell_type":"markdown","source":["# **Example 2: การทำ Pipelines ในการฝึกและทำนายโมเดล Logistic Regression**"],"metadata":{"id":"WR4ITxd1X8x0"}},{"cell_type":"markdown","source":["## Ex 2.1: ข้อมูลไม่ได้อยู่ในรูปของ Vector"],"metadata":{"id":"u5JkZV3DX9_B"}},{"cell_type":"markdown","source":["**STEP 1 : ใช้ Pipeline ในการสร้างกระบวนการ**"],"metadata":{"id":"s6HbUuADPahj"}},{"cell_type":"markdown","source":["สร้าง Pipeline ที่ประกอบด้วยหลายๆ ขั้นตอน เช่น Tokenizer, HashingTF และ LogisticRegression"],"metadata":{"id":"Ll590T2EPgQG"}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n","from pyspark.ml.feature import Tokenizer, HashingTF\n","\n","# เตรียมข้อมูลฝึกจำนวน 3 column\n","training = spark.createDataFrame([\n","    (0, \"a b c d e spark\", 1.0),\n","    (1, \"b d\", 0.0),\n","    (2, \"spark f g h\", 1.0),\n","    (3, \"hadoop mapreduce\", 0.0)\n","], [\"id\", \"text\", \"label\"])"],"metadata":{"id":"6CZYhjERYBm7","executionInfo":{"status":"ok","timestamp":1738037227853,"user_tz":-420,"elapsed":428,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}}},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":["Pipeline - Train Model"],"metadata":{"id":"Ipb9-4a2Fxus"}},{"cell_type":"code","source":["# กำหนด stage in pipeline\n","tokenizer = Tokenizer(inputCol = \"text\", outputCol = \"words\")\n","hashingTF = HashingTF(inputCol = tokenizer.getOutputCol(), outputCol = \"features\")\n","# tokenizer.getOutputCol() คำสั่งนี้ safe หน่อย\n","lr = LogisticRegression(maxIter = 10, regParam = 0.001)\n","\n","# Pipeline\n","pipeline = Pipeline(stages=[tokenizer,hashingTF, lr])\n","\n","# Pipeline Training (ฝึก model)\n","model = pipeline.fit(training) # pipeline เป็น estimator"],"metadata":{"id":"bzn62x8GYCfl","executionInfo":{"status":"ok","timestamp":1738037240120,"user_tz":-420,"elapsed":9373,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["type(model)"],"metadata":{"id":"ZSHoM4hnPigo","colab":{"base_uri":"https://localhost:8080/","height":156},"executionInfo":{"status":"ok","timestamp":1738037255787,"user_tz":-420,"elapsed":387,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}},"outputId":"a1ef9eab-2895-4cd5-a8ef-16e0c94aaf80"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["pyspark.ml.pipeline.PipelineModel"],"text/html":["<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n","      pre.function-repr-contents {\n","        overflow-x: auto;\n","        padding: 8px 12px;\n","        max-height: 500px;\n","      }\n","\n","      pre.function-repr-contents.function-repr-contents-collapsed {\n","        cursor: pointer;\n","        max-height: 100px;\n","      }\n","    </style>\n","    <pre style=\"white-space: initial; background:\n","         var(--colab-secondary-surface-color); padding: 8px 12px;\n","         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.ml.pipeline.PipelineModel</b><br/>def __init__(stages: List[Transformer])</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.11/dist-packages/pyspark/ml/pipeline.py</a>Represents a compiled pipeline with transformers and fitted models.\n","\n",".. versionadded:: 1.3.0</pre>\n","      <script>\n","      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n","        for (const element of document.querySelectorAll('.filepath')) {\n","          element.style.display = 'block'\n","          element.onclick = (event) => {\n","            event.preventDefault();\n","            event.stopPropagation();\n","            google.colab.files.view(element.textContent, 290);\n","          };\n","        }\n","      }\n","      for (const element of document.querySelectorAll('.function-repr-contents')) {\n","        element.onclick = (event) => {\n","          event.preventDefault();\n","          event.stopPropagation();\n","          element.classList.toggle('function-repr-contents-collapsed');\n","        };\n","      }\n","      </script>\n","      </div>"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["**STEP 2 : ทำนายผลด้วย Pipeline**"],"metadata":{"id":"kCoSnjYEPuMq"}},{"cell_type":"code","source":["# เตรียมข้อมูล test ประกอบด้วย 2 column\n","test = spark.createDataFrame([\n","    (4, \"spark i j k\"),\n","    (5, \"l m n\"),\n","    (6, \"spark hadoop spark\"),\n","    (7, \"apache hadoop\")\n","], [\"id\", \"text\"])\n"],"metadata":{"id":"-rOhXIHiQk_t","executionInfo":{"status":"ok","timestamp":1738037359185,"user_tz":-420,"elapsed":374,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}}},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":["Pipeline - Test Model"],"metadata":{"id":"zAjK3m1xGa7I"}},{"cell_type":"code","source":["prediction = model.transform(test) # เรียก pipeline บน test data\n","selected = prediction.select('id', 'text', 'probability', 'prediction')\n","for row in selected.collect():\n","  print('(%d, %s) --> prob = %s, prediction = %f' % (row.id, row.text, row.probability, row.prediction))"],"metadata":{"id":"0QRy8FsHYSKL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738037407007,"user_tz":-420,"elapsed":1723,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}},"outputId":"8f6c5ff6-94c4-42f9-c5c7-04c22f369128"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["(4, spark i j k) --> prob = [0.6292098489668483,0.3707901510331517], prediction = 0.000000\n","(5, l m n) --> prob = [0.984770006762304,0.015229993237696027], prediction = 0.000000\n","(6, spark hadoop spark) --> prob = [0.13412348342566147,0.8658765165743385], prediction = 1.000000\n","(7, apache hadoop) --> prob = [0.9955732114398529,0.00442678856014711], prediction = 0.000000\n"]}]},{"cell_type":"markdown","source":["## Ex 2.2: แปลงข้อมูลอยู่ในรูปของ Vector"],"metadata":{"id":"5HFZnufxYVAv"}},{"cell_type":"markdown","source":["ข้อมูลตัวอย่างประกอบด้วย column ต่างๆ เช่น\n","- label\n","- category (ประเภท categorical)\n","- feature1, feature2 (คุณสมบัติหรือฟีเจอร์เชิงตัวเลข)"],"metadata":{"id":"Rj7gk2cgYV7p"}},{"cell_type":"markdown","source":["**STEP 1 : สร้าง Spark Session**"],"metadata":{"id":"KCMNZF9nyrSV"}},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName(\"PipelineExample\").getOrCreate()"],"metadata":{"id":"i9yBSBOrzM06","executionInfo":{"status":"ok","timestamp":1738038877060,"user_tz":-420,"elapsed":402,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}}},"execution_count":52,"outputs":[]},{"cell_type":"markdown","source":["**STEP 2 : สร้าง DataFrame สำหรับการฝึกโมเดล**"],"metadata":{"id":"268RZQOxzWtO"}},{"cell_type":"code","source":["data = spark.createDataFrame([\n","    (0, \"a\", 1.0, 2.0),\n","    (1, \"b\", 2.0, 1.0),\n","    (0, \"a\", 2.0, 3.0),\n","    (1, \"b\", 1.0, 2.0),\n","], [\"label\", \"category\", \"feature1\", \"feature2\"])"],"metadata":{"id":"z3G9aVQtzmgz","executionInfo":{"status":"ok","timestamp":1738038878158,"user_tz":-420,"elapsed":4,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}}},"execution_count":53,"outputs":[]},{"cell_type":"markdown","source":["**STEP 3 : การแปลงข้อมูล - StringIndexer และ VectorAssembler**"],"metadata":{"id":"CzKxvGDOzq9k"}},{"cell_type":"markdown","source":["- StringIndexer: ใช้ในการแปลงค่าที่เป็นประเภท categorical (เช่น \"a\", \"b\") ให้เป็นตัวเลข\n","\n","- VectorAssembler: ใช้ในการรวมฟีเจอร์หลายๆ คอลัมน์ให้เป็นคอลัมน์เดียวที่สามารถนำไปใช้ฝึกโมเดลได้"],"metadata":{"id":"gdE0jBY0zxLX"}},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer, VectorAssembler\n","\n","indexer = StringIndexer(inputCol=\"category\", outputCol='categoryIndex',\n","                        stringOrderType='frequencyDesc')\n","assemble = VectorAssembler(inputCols=[\"feature1\", \"feature2\", \"categoryIndex\"],\n","                           outputCol=\"features\")"],"metadata":{"id":"06lHh-zpz_RN","executionInfo":{"status":"ok","timestamp":1738038879572,"user_tz":-420,"elapsed":7,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}}},"execution_count":54,"outputs":[]},{"cell_type":"markdown","source":["**STEP 4 : การเลือกโมเดล - Logistic Regression**"],"metadata":{"id":"_b0UyYLW0ueR"}},{"cell_type":"markdown","source":["ในขั้นตอนนี้ เราจะเลือกโมเดล Logistic Regression เพื่อใช้ในการฝึกโมเดลสำหรับการจำแนกประเภท"],"metadata":{"id":"EtHD3GWB02xJ"}},{"cell_type":"code","source":["from pyspark.ml.classification import LogisticRegression\n","\n","lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\",\n","                        maxIter=1000, tol=0.0001, regParam=0.01)"],"metadata":{"id":"qIaUPkTW04ww","executionInfo":{"status":"ok","timestamp":1738038883174,"user_tz":-420,"elapsed":429,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}}},"execution_count":55,"outputs":[]},{"cell_type":"markdown","source":["**STEP 5 : การสร้าง Pipeline**"],"metadata":{"id":"3-KiAI091D6o"}},{"cell_type":"markdown","source":["ในขั้นตอนนี้ เราจะรวมขั้นตอนต่างๆ ไว้ใน Pipeline ซึ่งประกอบด้วย\n","\n","- การแปลงข้อมูลด้วย StringIndexer\n","- การรวมฟีเจอร์ด้วย VectorAssembler\n","- การฝึกโมเดลด้วย LogisticRegression"],"metadata":{"id":"t9p9N2wB1HhG"}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n","\n","pipeline = Pipeline(stages=[indexer, assemble, lr])"],"metadata":{"id":"Y-446pn71PUK","executionInfo":{"status":"ok","timestamp":1738038885999,"user_tz":-420,"elapsed":508,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}}},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":["**STEP 6 : การฝึกโมเดล**"],"metadata":{"id":"Uyft0Kul1WKa"}},{"cell_type":"markdown","source":["หลังจากสร้าง Pipeline เสร็จแล้ว เราจะใช้ fit() เพื่อฝึกโมเดลด้วยข้อมูลที่เตรียมไว้"],"metadata":{"id":"AuGkQMS21ZtI"}},{"cell_type":"code","source":["model = pipeline.fit(data)"],"metadata":{"id":"muGpx9WV1dm0","executionInfo":{"status":"ok","timestamp":1738038890243,"user_tz":-420,"elapsed":2827,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}}},"execution_count":57,"outputs":[]},{"cell_type":"markdown","source":["**STEP 7 : การทำนายผล (Prediction)**"],"metadata":{"id":"7rNTcNti1jKr"}},{"cell_type":"code","source":["predictions = model.transform(data)\n","\n","predictions.show()"],"metadata":{"id":"u6hkJMIa1m1h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738038891771,"user_tz":-420,"elapsed":1530,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}},"outputId":"4a9f2806-b3db-426d-b65d-6859844440e6"},"execution_count":58,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+--------+--------+--------+-------------+-------------+--------------------+--------------------+----------+\n","|label|category|feature1|feature2|categoryIndex|     features|       rawPrediction|         probability|prediction|\n","+-----+--------+--------+--------+-------------+-------------+--------------------+--------------------+----------+\n","|    0|       a|     1.0|     2.0|          0.0|[1.0,2.0,0.0]|[2.79474715118196...|[0.94239130912281...|       0.0|\n","|    1|       b|     2.0|     1.0|          1.0|[2.0,1.0,1.0]|[-4.0629416230743...|[0.01690757117189...|       1.0|\n","|    0|       a|     2.0|     3.0|          0.0|[2.0,3.0,0.0]|[4.06294162307433...|[0.98309242882810...|       0.0|\n","|    1|       b|     1.0|     2.0|          1.0|[1.0,2.0,1.0]|[-2.7947471511819...|[0.05760869087718...|       1.0|\n","+-----+--------+--------+--------+-------------+-------------+--------------------+--------------------+----------+\n","\n"]}]},{"cell_type":"markdown","source":["Model Description"],"metadata":{"id":"jbmgsyqOYw9h"}},{"cell_type":"code","source":["model.coefficients"],"metadata":{"id":"jIIki-lbYxeV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.intercept"],"metadata":{"id":"JZ0XvP_sYxsn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vFfah86iI3_-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model Evaluation"],"metadata":{"id":"v0BLyP_-YzY6"}},{"cell_type":"code","source":["model_test_result = model.evaluate(test)\n","\n","model_test_result"],"metadata":{"id":"cdOD-U88Yz5V","colab":{"base_uri":"https://localhost:8080/","height":183},"executionInfo":{"status":"error","timestamp":1738038958019,"user_tz":-420,"elapsed":402,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}},"outputId":"d7e5f8fd-b972-44b4-c618-b19bdea8e601"},"execution_count":62,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"'PipelineModel' object has no attribute 'evaluate'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-62-46b66031b989>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_test_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_test_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'PipelineModel' object has no attribute 'evaluate'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"c3Cr1GmdY0SW","executionInfo":{"status":"ok","timestamp":1738038993254,"user_tz":-420,"elapsed":411,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}}},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":["# **Extracting, transforming and selecting features**"],"metadata":{"id":"Vb1bZQqZs69B"}},{"cell_type":"markdown","source":["Algorithm ต่างๆ ที่ใช้ในการทำงานกับ feature สามารถแบ่งออกเป็นกลุ่มต่างๆ ดังนี้\n","\n","- **Extraction**\n","- **Transformation**\n","- **Selection**\n"],"metadata":{"id":"gnJfRBJ-5d3Z"}},{"cell_type":"markdown","source":["### **Feature Extractors**"],"metadata":{"id":"F2ohw3szmWOV"}},{"cell_type":"markdown","source":["Feature Extractors คือ กระบวนการที่ใช้ในการดึงเอาคุณสมบัติ (features) ที่สำคัญจากข้อมูลในรูปแบบดิบ (raw data) เพื่อนำมาประมวลผลและใช้ในการฝึกโมเดล Machine Learning. การใช้ Feature Extractors ช่วยให้สามารถคัดเลือกข้อมูลที่มีความหมายและเป็นประโยชน์สำหรับการทำนายหรือการจำแนกประเภทได้ดีขึ้น ลดความซับซ้อนของข้อมูลและเพิ่มประสิทธิภาพในการเรียนรู้ของโมเดล"],"metadata":{"id":"x4QRUPvemkA3"}},{"cell_type":"markdown","source":["**1. TF-IDF (Term Frequency-Inverse Document Frequency)**\n"],"metadata":{"id":"mTKU-uThm5Vj"}},{"cell_type":"markdown","source":["เป็นวิธีการแปลงข้อมูลจากข้อความให้เป็นเวกเตอร์ที่สะท้อนความสำคัญของคำในเอกสาร โดยวิธีนี้จะช่วยให้สามารถแยกแยะคำที่สำคัญและไม่สำคัญในเอกสารนั้นได้\n","\n","  หลักการของ TF-IDF ประกอบไปด้วย 2 ส่วนหลัก\n","  \n","    1. Term Frequency (TF): คือจำนวนครั้งที่คำ t ปรากฏในเอกสาร d\n","    2. Inverse Document Frequency (IDF): คือค่าที่วัดความสำคัญของคำ t ในฐานข้อมูล (corpus) โดยจะคำนวณจากจำนวนเอกสารที่คำนี้ปรากฏ (DF) ในฐานข้อมูลทั้งหมด D"],"metadata":{"id":"Gf18EGXH4LzW"}},{"cell_type":"code","source":["from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.appName(\"TF-IDF Example\").getOrCreate()\n","\n","sentenceData = spark.createDataFrame([\n","    (0.0, \"Hi I heard about Spark\"),\n","    (0.0, \"I wish Java could use case classes\"),\n","    (1.0, \"Logistic regression models are neat\")\n","], [\"label\", \"sentence\"])\n","\n","tokenizer = Tokenizer(inputCol='sentence', outputCol=\"words\")\n","hashTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"rawFeatures\", numFeatures=20)\n","idf = IDF(inputCol=hashTF.getOutputCol(), outputCol=\"features\")"],"metadata":{"id":"YL6aCb5jnGUM","executionInfo":{"status":"ok","timestamp":1738039189385,"user_tz":-420,"elapsed":362,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}}},"execution_count":63,"outputs":[]},{"cell_type":"code","source":["pipeline = Pipeline(stages=[tokenizer, hashTF, idf])\n","model = pipeline.fit(sentenceData)\n","predictions = model.transform(sentenceData)"],"metadata":{"id":"3ij2e3AUNVJ3","executionInfo":{"status":"ok","timestamp":1738039293711,"user_tz":-420,"elapsed":1489,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","source":["predictions.select('label', 'features').show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o_6i1MwXNVVR","executionInfo":{"status":"ok","timestamp":1738039376856,"user_tz":-420,"elapsed":859,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}},"outputId":"a7156093-0bfc-43cb-8a77-aca056a90458"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+--------------------+\n","|label|            features|\n","+-----+--------------------+\n","|  0.0|(20,[6,8,13,16],[...|\n","|  0.0|(20,[0,2,7,13,15,...|\n","|  1.0|(20,[3,4,6,11,19]...|\n","+-----+--------------------+\n","\n"]}]},{"cell_type":"markdown","source":["**2. Word2Vec**"],"metadata":{"id":"N6HXNtMxqp5t"}},{"cell_type":"markdown","source":["เป็น Estimator ที่ใช้สำหรับแปลงคำแต่ละคำในเอกสารให้เป็นเวกเตอร์ขนาดคงที่ โดยการเรียนรู้จากชุดข้อมูลที่เป็นประโยคหรือเอกสารที่มีการแยกคำ (bag of words) ซึ่งจะช่วยในการแปลงคำในเอกสารให้เป็นเวกเตอร์เพื่อใช้ในงานต่างๆ เช่น การคำนวณความคล้ายคลึงของเอกสาร หรือใช้เป็นคุณลักษณะ (feature) สำหรับการพยากรณ์"],"metadata":{"id":"AOiWsuln4VRZ"}},{"cell_type":"code","source":["from pyspark.ml.feature import Word2Vec\n","\n","documentDF = spark.createDataFrame([\n","    (\"Hi I heard about Spark\".split(\" \"), ),\n","    (\"I wish Java could use case classes\".split(\" \"), ),\n","    (\"Logistic regression models are neat\".split(\" \"), )\n","], [\"text\"])\n","\n","word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\") # Estimator\n","model = word2Vec.fit(documentDF)\n","result = model.transform(documentDF)\n","\n","for row in result.collect():\n","  text, vector = row\n","  print(\"text: %s -> \\nVector: %s \\n\" %(\", \".join(text), vector))"],"metadata":{"id":"IREs5B82n3tM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738039629279,"user_tz":-420,"elapsed":3559,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}},"outputId":"160d2c34-1c7a-447b-fed8-771f90596e5d"},"execution_count":67,"outputs":[{"output_type":"stream","name":"stdout","text":["text: Hi, I, heard, about, Spark -> \n","Vector: [-0.026580248773097993,-0.02676662188023329,0.04222403690218926] \n","\n","text: I, wish, Java, could, use, case, classes -> \n","Vector: [-0.034757427397250594,-0.05261531685079847,-0.0004108633313860212] \n","\n","text: Logistic, regression, models, are, neat -> \n","Vector: [0.013996277935802937,0.04871041178703309,-0.03697966188192368] \n","\n"]}]},{"cell_type":"markdown","source":["**3. CountVectorizer**"],"metadata":{"id":"0LrcS-PevTh4"}},{"cell_type":"markdown","source":["คือเครื่องมือที่ใช้แปลงข้อความเป็นเวกเตอร์ที่แสดงจำนวนการปรากฏของคำ (token counts) ในแต่ละเอกสาร โดยไม่จำเป็นต้องมีพจนานุกรมล่วงหน้า (a-priori dictionary) ด้วยการใช้ Estimator ในการสร้างพจนานุกรมจากข้อมูลที่มีอยู่ โมเดล CountVectorizerModel ที่ได้จากการฝึก (fitting) จะสร้างการแทนเอกสารในรูปแบบเวกเตอร์ที่แสดงการปรากฏของคำในพจนานุกรมที่ถูกสร้างขึ้น\n","\n","  ในระหว่างการฝึกโมเดล, CountVectorizer จะเลือกคำที่มีความถี่สูงที่สุดในข้อมูล และกำหนดพารามิเตอร์ต่างๆ เช่น\n","\n","  - vocabSize: ขนาดของพจนานุกรมที่ใช้\n","\n","  - minDF: กำหนดจำนวนเอกสารขั้นต่ำที่คำต้องปรากฏถึงจะถูกใช้ในพจนานุกรม\n","  \n","  - binary: หากตั้งค่าเป็น true จะใช้เวกเตอร์แบบทวิภาค (binary vector) ซึ่งจะตั้งค่าที่ไม่เป็นศูนย์ทั้งหมดเป็น 1 (เหมาะกับโมเดลที่ทำงานกับข้อมูลแบบทวิภาค)"],"metadata":{"id":"gGePrgE-4f6H"}},{"cell_type":"code","source":["from pyspark.ml.feature import CountVectorizer\n","\n","df = spark.createDataFrame([\n","    (0, \"a b c\".split(\" \")),\n","    (1, \"a b b c a\".split(\" \"))\n","], [\"id\", \"words\"])"],"metadata":{"id":"9juF5l3PwsDn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**4. FeatureHasher**"],"metadata":{"id":"5iT2lMZBx8CH"}},{"cell_type":"markdown","source":["เป็นเทคนิคการแปลงข้อมูลที่ใช้การแฮชเพื่อทำให้ชุดของฟีเจอร์ที่มีลักษณะทั้งประเภทหมวดหมู่ (categorical) และตัวเลข (numerical) ถูกแปลงเป็นเวกเตอร์ฟีเจอร์ในมิติที่กำหนด (โดยปกติจะมีขนาดเล็กกว่าพื้นที่ฟีเจอร์เดิมมาก) โดยใช้ \"hashing trick\" ในการจับคู่ฟีเจอร์กับตำแหน่งในเวกเตอร์\n","\n","  ลักษณะการทำงานของ FeatureHasher กับคอลัมน์ประเภทต่างๆ ดังนี้\n","\n","  - **Numeric columns** จะใช้ค่าแฮชของชื่อคอลัมน์เพื่อจับคู่ค่าฟีเจอร์กับตำแหน่งในเวกเตอร์ฟีเจอร์ โดยปกติแล้วฟีเจอร์ตัวเลขจะไม่ถูกมองเป็นฟีเจอร์หมวดหมู่ (ถึงแม้ว่าจะเป็นจำนวนเต็ม) แต่สามารถระบุให้เป็นฟีเจอร์หมวดหมู่ได้ด้วยการกำหนดคอลัมน์ที่ต้องการในพารามิเตอร์ categoricalCols\n","\n","  - **String columns** ฟีเจอร์ประเภทนี้จะใช้ค่าของการแฮชจากการรวมชื่อคอลัมน์และค่าของคอลัมน์ (เช่น \"column_name=value\") เพื่อจับคู่กับตำแหน่งในเวกเตอร์ โดยใช้ค่าตัวชี้ (indicator value) เป็น 1.0 คล้ายกับการทำ one-hot encoding\n","\n","  - **Boolean columns** ฟีเจอร์ประเภทนี้จะทำงานเหมือนกับคอลัมน์สตริง โดยใช้ค่าแฮชจาก \"column_name=true\" หรือ \"column_name=false\" กับค่าตัวชี้ 1.0\n","\n","  - **ค่าที่หายไป (null)** ค่าที่หายไปจะถูกละเว้นในการแปลง (จะถือเป็นค่า 0 ในเวกเตอร์ฟีเจอร์)"],"metadata":{"id":"-OrVT2Ht4mZn"}},{"cell_type":"code","source":["from pyspark.ml.feature import FeatureHasher\n","\n","df = spark.createDataFrame([\n","    (2.2, True, \"1\", \"foo\"),\n","    (3.3, False, \"2\", \"bar\"),\n","    (4.4, False, \"3\", \"baz\"),\n","    (5.5, False, \"4\", \"foo\")\n","], [\"real\", \"bool\", \"stringNum\", \"string\"])"],"metadata":{"id":"kyDjpU4s0Zyz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Feature Transformers**"],"metadata":{"id":"_JzScSc_43_Z"}},{"cell_type":"markdown","source":["**Feature Transformers** คือ กระบวนการที่ใช้ในการแปลงหรือเปลี่ยนแปลงคุณสมบัติ (features) ของข้อมูล เพื่อให้เหมาะสมกับการนำไปใช้ในโมเดล Machine Learning การแปลงนี้สามารถช่วยให้โมเดลเรียนรู้ข้อมูลได้ดีขึ้น หรือสามารถปรับข้อมูลให้เป็นมาตรฐานที่เหมาะสมกับอัลกอริธึมที่ใช้\n","\n","ใน PySpark ฟังก์ชัน transform เป็นฟังก์ชันที่ใช้สำหรับทำการแปลงข้อมูล (transform data) หลังจากที่โมเดลหรือ Pipeline ถูกฝึกแล้ว (fit) เพื่อให้โมเดลสามารถนำข้อมูลใหม่ไปใช้ในการทำนายผลหรือการประมวลผลต่างๆ ได้\n","\n","**เมื่อไหร่ที่เราจะใช้ transform?**\n","\n","โดยปกติแล้ว หลังจากที่เราฝึกโมเดลหรือ Pipeline ด้วย fit() เสร็จแล้ว เราจะใช้ transform() เพื่อนำข้อมูลใหม่ (ที่อาจจะไม่เคยเห็นมาก่อน) ไปผ่านขั้นตอนที่ได้ทำการฝึกไว้แล้ว เช่น การแปลงข้อมูลและการทำนายผลลัพธ์"],"metadata":{"id":"89uaWxo1YxyN"}},{"cell_type":"markdown","source":["**1. Tokenizer & RegexTokenizer**"],"metadata":{"id":"STDl32T_5Tcz"}},{"cell_type":"markdown","source":[" ใช้ในการแยกคำจากข้อความ โดย Tokenizer จะใช้การแยกคำตามช่องว่าง ส่วน RegexTokenizer ใช้การแยกคำโดยใช้ regular expressions (regex) เพื่อให้สามารถปรับแต่งวิธีการแยกคำได้มากขึ้น"],"metadata":{"id":"UWvEuGHmArUF"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"ihJ6DxmUbOCu"}},{"cell_type":"code","source":["from pyspark.ml.feature import Tokenizer\n","from pyspark.sql.functions import col, udf\n","from pyspark.sql.types import IntegerType\n","\n","df = spark.createDataFrame([\n","    (0, \"Hi I heard about Spark\"),\n","    (1, \"I wish Java could use case classes\"),\n","    (2, \"Logistic,regression,models,are,neat\")\n","], [\"id\", \"sentence\"])"],"metadata":{"id":"06dRkXQbAzgT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**2. StopWordsRemover**"],"metadata":{"id":"x0valT_i5Xtj"}},{"cell_type":"markdown","source":["ใช้ในการลบคำที่ไม่สำคัญ (stop words) เช่น \"I\", \"the\", \"a\" ออกจากชุดข้อมูลเพื่อลดความซับซ้อนและเพิ่มประสิทธิภาพ"],"metadata":{"id":"n947EgQEBKrt"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"4Xjyp793bNSk"}},{"cell_type":"code","source":["from pyspark.ml.feature import StopWordsRemover\n","\n","df = spark.createDataFrame([\n","    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n","    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n","], [\"id\", \"raw\"])\n","\n","remover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\n","remover.transform(df).show(truncate=False)"],"metadata":{"id":"F604SI11BBPY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738039817166,"user_tz":-420,"elapsed":1320,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}},"outputId":"56ceee7d-5069-4fb4-a8c0-931b23dd96e9"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+----------------------------+--------------------+\n","|id |raw                         |filtered            |\n","+---+----------------------------+--------------------+\n","|0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n","|1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n","+---+----------------------------+--------------------+\n","\n"]}]},{"cell_type":"markdown","source":["**3. NGram**"],"metadata":{"id":"3wTJUSxEBkSB"}},{"cell_type":"markdown","source":["ใช้ในการสร้างชุดคำ n-gram ซึ่งเป็นการจับคู่ของคำหลายๆ คำ (เช่น 2 คำ หรือ 3 คำ) เพื่อใช้ในงานที่ต้องการตรวจจับลำดับของคำ"],"metadata":{"id":"PSzvvod7BqOQ"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"y2LuFME8bMwC"}},{"cell_type":"code","source":["from pyspark.ml.feature import NGram\n","\n","df = spark.createDataFrame([\n","    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n","    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n","    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n","], [\"id\", \"words\"])"],"metadata":{"id":"luvJxb66BxTD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**4. Binarizer**"],"metadata":{"id":"Q7gOaC7vBvhE"}},{"cell_type":"markdown","source":["ใช้ในการแปลงข้อมูลเชิงตัวเลขให้กลายเป็นข้อมูลแบบไบนารี (0 หรือ 1) โดยอาศัยเกณฑ์ threshold ในการตัดสินใจ"],"metadata":{"id":"RUpCiJxFB1RH"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"AJdHqFlBbMAe"}},{"cell_type":"code","source":["from pyspark.ml.feature import Binarizer\n","\n","df = spark.createDataFrame([\n","    (0, 0.1),\n","    (1, 0.8),\n","    (2, 0.2)\n","], [\"id\", \"feature\"])"],"metadata":{"id":"8OkD7eIfBr2O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**5. PCA (Principal Component Analysis)**"],"metadata":{"id":"d4rPuKkJC5z5"}},{"cell_type":"markdown","source":["ใช้ในการลดมิติของข้อมูลจากหลายมิติให้เหลือน้อยลง โดยคำนวณเป็นองค์ประกอบหลัก (Principal Components)"],"metadata":{"id":"6GWUofHwC7rT"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"4uDJS9j6bLTj"}},{"cell_type":"code","source":["from pyspark.ml.feature import PCA\n","from pyspark.ml.linalg import Vectors\n","\n","df = spark.createDataFrame([(Vectors.sparse(5, [(1, 1.0), (3, 7.0)]),),\n","                            (Vectors.dense([2.0, 0.0, 3.0, 4.0, 5.0]),),\n","                            (Vectors.dense([4.0, 0.0, 0.0, 6.0, 7.0]),)], [\"features\"])"],"metadata":{"id":"4KojN8hGDGpZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**6. PolynomialExpansion**"],"metadata":{"id":"GkcP2v7DDkhr"}},{"cell_type":"markdown","source":["ใช้ในการขยายข้อมูลในรูปแบบของพหุนาม โดยการสร้างคุณลักษณะใหม่ๆ ที่เป็นการผสมผสานระหว่างคุณลักษณะเดิม"],"metadata":{"id":"XbqAMtrpEtMH"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"FuI7bdl5bKgS"}},{"cell_type":"code","source":["from pyspark.ml.feature import PolynomialExpansion\n","from pyspark.ml.linalg import Vectors\n","\n","df = spark.createDataFrame([\n","    (Vectors.dense([2.0, 1.0]),),\n","    (Vectors.dense([0.0, 0.0]),),\n","    (Vectors.dense([3.0, -1.0]),)\n","], [\"features\"])\n"],"metadata":{"id":"KL3BOQHzDGl7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**7. Discrete Cosine Transform (DCT)**"],"metadata":{"id":"ljdLyumOFMXi"}},{"cell_type":"markdown","source":["ใช้ในการแปลงข้อมูลจากโดเมนเวลาไปยังโดเมนความถี่ เพื่อใช้ในงานประมวลผลสัญญาณ"],"metadata":{"id":"jYNFmHcRE4sD"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"ysB6xEZ7bJzA"}},{"cell_type":"code","source":["from pyspark.ml.feature import DCT\n","from pyspark.ml.linalg import Vectors\n","\n","df = spark.createDataFrame([\n","    (Vectors.dense([0.0, 1.0, -2.0, 3.0]),),\n","    (Vectors.dense([-1.0, 2.0, 4.0, -7.0]),),\n","    (Vectors.dense([14.0, -2.0, -5.0, 1.0]),)\n","], [\"features\"])"],"metadata":{"id":"JoZ0uaSmE5rs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**8. StringIndexer**"],"metadata":{"id":"hP47KnyqE6KS"}},{"cell_type":"markdown","source":["ใช้ในการแปลงค่าของตัวแปรที่เป็นข้อความ (string) ให้เป็นตัวเลข ซึ่งมักใช้ในกรณีที่ข้อมูลเป็นหมวดหมู่ (categorical)"],"metadata":{"id":"L9OaxFK_FWd4"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"a_73mpr7bI_q"}},{"cell_type":"code","source":["from pyspark.ml.feature import StringIndexer\n","\n","df = spark.createDataFrame(\n","    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n","    [\"id\", \"category\"]\n",")"],"metadata":{"id":"R-CIK0djFZKr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**9. IndexToString**"],"metadata":{"id":"ijIEYxTzFZtk"}},{"cell_type":"markdown","source":["ใช้ในการแปลงค่าตัวเลขที่ได้จาก StringIndexer กลับเป็นข้อความเดิม"],"metadata":{"id":"-JcMc3R5FhPn"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"fNgw51uAbISO"}},{"cell_type":"code","source":["from pyspark.ml.feature import IndexToString, StringIndexer\n","\n","df = spark.createDataFrame(\n","    [(0, \"a\"), (1, \"b\"), (2, \"c\"), (3, \"a\"), (4, \"a\"), (5, \"c\")],\n","    [\"id\", \"category\"])"],"metadata":{"id":"7HjqZ0xrFiP4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**10. OneHotEncoder**"],"metadata":{"id":"nK4SNpd9FkGT"}},{"cell_type":"markdown","source":["ใช้ในการแปลงค่าหมวดหมู่ (categorical values) ให้เป็นเวกเตอร์ไบนารีที่มีค่า 1 สำหรับหมวดหมู่ที่เลือก และค่า 0 สำหรับหมวดหมู่อื่นๆ"],"metadata":{"id":"OgUqBrVLFpVS"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"C5W5hii6bHNz"}},{"cell_type":"code","source":["from pyspark.ml.feature import OneHotEncoder\n","\n","df = spark.createDataFrame([\n","    (0.0, 1.0),\n","    (1.0, 0.0),\n","    (2.0, 1.0),\n","    (0.0, 2.0),\n","    (0.0, 1.0),\n","    (2.0, 0.0)\n","], [\"categoryIndex1\", \"categoryIndex2\"])"],"metadata":{"id":"zgRYd809Fs6d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**11. VectorIndexer**"],"metadata":{"id":"l_Wy9zh4FvtI"}},{"cell_type":"markdown","source":["ใช้ในการจัดการกับคุณลักษณะประเภทหมวดหมู่ในชุดข้อมูลที่อยู่ในรูปแบบเวกเตอร์ (Vector)"],"metadata":{"id":"ml4Oeby2FzZ5"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"0s31Xq6EbGfb"}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorIndexer\n","from pyspark.ml.linalg import Vectors\n","\n","df = spark.createDataFrame([\n","    (0, Vectors.dense([1.0, 0.1, 0.2]),),\n","    (1, Vectors.dense([2.0, 1.1, 1.2]),),\n","    (2, Vectors.dense([3.0, 10.1, 10.2]),),\n","    (3, Vectors.dense([4.0, 100.1, 100.2]),)\n","], [\"id\", \"features\"])"],"metadata":{"id":"u_RzPuhhFxh7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**12. Interaction**"],"metadata":{"id":"Xzb4CmmHF0Yy"}},{"cell_type":"markdown","source":["ใช้ในการคำนวณการมีปฏิสัมพันธ์ (interaction) ของคุณลักษณะหลายๆ ตัว โดยการสร้างเวกเตอร์ที่ประกอบไปด้วยผลคูณของค่าต่างๆ"],"metadata":{"id":"yzryaxXxF488"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"7oMDpWRRbF0q"}},{"cell_type":"code","source":["from pyspark.ml.feature import Interaction, VectorAssembler\n","\n","df = spark.createDataFrame(\n","    [(1, 1, 2, 3, 8, 4, 5),\n","     (2, 4, 3, 8, 7, 9, 8),\n","     (3, 6, 1, 9, 2, 3, 6),\n","     (4, 10, 8, 6, 9, 4, 5),\n","     (5, 9, 2, 7, 10, 7, 3),\n","     (6, 1, 1, 4, 2, 8, 4)],\n","    [\"id1\", \"id2\", \"id3\", \"id4\", \"id5\", \"id6\", \"id7\"])"],"metadata":{"id":"oM5Mq5XvF25C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**13. Normalizer**"],"metadata":{"id":"QoMJ90SPF7qT"}},{"cell_type":"markdown","source":["ใช้ในการปรับขนาดเวกเตอร์ให้มีความยาวเท่ากับ 1 โดยใช้การทำ normalization ซึ่งช่วยปรับมาตรฐานของข้อมูลให้เหมาะสมกับการเรียนรู้ของเครื่อง"],"metadata":{"id":"q-orUJW_GCO7"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"OjyHJoSWbFGL"}},{"cell_type":"code","source":["from pyspark.ml.feature import Normalizer\n","from pyspark.ml.linalg import Vectors\n","\n","df = spark.createDataFrame([\n","    (0, Vectors.dense([1.0, 0.5, -1.0]),),\n","    (1, Vectors.dense([2.0, 1.0, 1.0]),),\n","    (2, Vectors.dense([4.0, 10.0, 2.0]),)\n","], [\"id\", \"features\"])"],"metadata":{"id":"Ghxz5_5FGDM8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**14. StandardScaler**"],"metadata":{"id":"ZFHYxvyTMmMp"}},{"cell_type":"markdown","source":["ปรับขนาดข้อมูลโดยทำให้ค่าเฉลี่ยเป็นศูนย์และส่วนเบี่ยงเบนมาตรฐานเป็น 1 เหมาะสำหรับข้อมูลที่มีการกระจายเป็นปกติ โดยตั้งค่าหลัก withStd=True และ withMean=False"],"metadata":{"id":"r_7OgErXM2Sg"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"HU7hB3W8bEUr"}},{"cell_type":"code","source":["from pyspark.ml.feature import StandardScaler\n","from pyspark.ml.linalg import Vectors\n","\n","df = spark.createDataFrame([(0, Vectors.dense([1.0, 0.1, -1.0])),\n"," (1, Vectors.dense([2.0, 1.1, 1.0])),\n","  (2, Vectors.dense([3.0, 10.1, 3.0]))], [\"id\", \"features\"])\n","\n","scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n","scalerModel = scaler.fit(df)\n","scaledData = scalerModel.transform(df)\n","scaledData.select(\"id\", \"features\", \"scaledFeatures\").show(truncate=False)"],"metadata":{"id":"bHZ8S4htPCE6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738040014698,"user_tz":-420,"elapsed":1722,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}},"outputId":"5375bb25-33ed-408b-a1a5-b2dce32233f3"},"execution_count":70,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+--------------+-------------------------------+\n","|id |features      |scaledFeatures                 |\n","+---+--------------+-------------------------------+\n","|0  |[1.0,0.1,-1.0]|[1.0,0.018156825980064073,-0.5]|\n","|1  |[2.0,1.1,1.0] |[2.0,0.19972508578070483,0.5]  |\n","|2  |[3.0,10.1,3.0]|[3.0,1.8338394239864713,1.5]   |\n","+---+--------------+-------------------------------+\n","\n"]}]},{"cell_type":"markdown","source":["**15. RobustScaler**"],"metadata":{"id":"YgPoxu1XO7QU"}},{"cell_type":"markdown","source":["ใช้ค่ามัธยฐานและช่วงควอนไทล์ (IQR) แทนค่าเฉลี่ยและส่วนเบี่ยงเบนมาตรฐาน ทำให้ทนทานต่อข้อมูลที่มีค่าผิดปกติ (outliers)"],"metadata":{"id":"YTC6ZdNuO8UF"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"fwqkkBwdbDsM"}},{"cell_type":"code","source":["from pyspark.ml.feature import RobustScaler\n","from pyspark.ml.linalg import Vectors\n","\n","df = spark.createDataFrame([(0, Vectors.dense([1.0, 0.1, -1.0])), (1, Vectors.dense([2.0, 1.1, 1.0])), (2, Vectors.dense([3.0, 10.1, 3.0]))], [\"id\", \"features\"])"],"metadata":{"id":"ZhXuZLrvM28s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**16. MinMaxScaler**"],"metadata":{"id":"q3W8U4K_PCy5"}},{"cell_type":"markdown","source":["ปรับขนาดข้อมูลให้อยู่ในช่วงที่กำหนด (เช่น [0, 1]) โดยพิจารณาจากค่าต่ำสุดและค่าสูงสุดของแต่ละฟีเจอร์"],"metadata":{"id":"ksIpTDAHPIhh"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"rRqbIYHybCxx"}},{"cell_type":"code","source":["from pyspark.ml.feature import MinMaxScaler\n","from pyspark.ml.linalg import Vectors\n","\n","df = spark.createDataFrame([\n","    (0, Vectors.dense([1.0, 0.1, -1.0]),),\n","    (1, Vectors.dense([2.0, 1.1, 1.0]),),\n","    (2, Vectors.dense([3.0, 10.1, 3.0]),)\n","], [\"id\", \"features\"])"],"metadata":{"id":"qSWX92fGPGHC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**17. MaxAbsScaler**"],"metadata":{"id":"hIA_h6FhPJ33"}},{"cell_type":"markdown","source":["ปรับขนาดข้อมูลในช่วง [-1, 1] โดยใช้ค่ามากสุดสัมบูรณ์ (maximum absolute value) ของแต่ละฟีเจอร์ และไม่ทำการศูนย์ค่าเฉลี่ย"],"metadata":{"id":"rO5nfIIRPNA4"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"rwFlIXYhbBuG"}},{"cell_type":"code","source":["from pyspark.ml.feature import MaxAbsScaler\n","from pyspark.ml.linalg import Vectors\n","\n","df = spark.createDataFrame([\n","    (0, Vectors.dense([1.0, 0.1, -8.0]),),\n","    (1, Vectors.dense([2.0, 1.0, -4.0]),),\n","    (2, Vectors.dense([4.0, 10.0, 8.0]),)\n","], [\"id\", \"features\"])"],"metadata":{"id":"hLpMgW9YPMuI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**18. Bucketizer**"],"metadata":{"id":"zS3U9J2JPPyJ"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"foBCebpHbA4e"}},{"cell_type":"markdown","source":["แบ่งข้อมูลเป็นกลุ่ม (bins) ตามช่วงที่กำหนด ใช้ในการแบ่งข้อมูลต่อเนื่องออกเป็นช่วงๆ"],"metadata":{"id":"4ALKLwkbPT03"}},{"cell_type":"code","source":["from pyspark.ml.feature import Bucketizer\n","\n","splits = [-float(\"inf\"), -0.5, 0.0, 0.5, float(\"inf\")]\n","\n","df = spark.createDataFrame([(-999.9,), (-0.5,), (-0.3,), (0.0,), (0.2,), (999.9,)], [\"features\"])"],"metadata":{"id":"KEsiogOVPWrS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**19. ElementwiseProduct**"],"metadata":{"id":"SBp7jwv5PXHJ"}},{"cell_type":"markdown","source":["คูณข้อมูลในแต่ละฟีเจอร์ด้วยเวกเตอร์ที่กำหนด ซึ่งเรียกว่าการคูณเชิงองค์ประกอบ (element-wise multiplication)"],"metadata":{"id":"r-ov67AiPaFS"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"VvWXS0H-a9ti"}},{"cell_type":"code","source":["from pyspark.ml.feature import ElementwiseProduct\n","from pyspark.ml.linalg import Vectors\n","\n","df = spark.createDataFrame([(Vectors.dense([1.0, 2.0, 3.0]),), (Vectors.dense([4.0, 5.0, 6.0]),)], [\"vector\"])"],"metadata":{"id":"FdBKqOTlPd1U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**20. SQLTransformer**"],"metadata":{"id":"2ajXymfZPdR7"}},{"cell_type":"markdown","source":["ใช้ SQL ในการแปลงข้อมูล เช่น การเลือกและแปลงข้อมูลโดยใช้คำสั่ง SELECT"],"metadata":{"id":"EHlDEDkBPqfJ"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"DVCDA6Sca6zO"}},{"cell_type":"code","source":["from pyspark.ml.feature import SQLTransformer\n","\n","df = spark.createDataFrame([\n","    (0, 1.0, 3.0),\n","    (2, 2.0, 5.0)\n","], [\"id\", \"v1\", \"v2\"])"],"metadata":{"id":"IFUzw-GTPoNa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**21. VectorAssembler**"],"metadata":{"id":"Q66lglYkPs_o"}},{"cell_type":"markdown","source":["รวมหลายคอลัมน์เข้าด้วยกันเป็นเวกเตอร์เดียว ใช้ในขั้นตอนการเตรียมข้อมูลสำหรับการเรียนรู้ของเครื่อง"],"metadata":{"id":"0fYUqs0RPv_d"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"O3TsYczGa5tE"}},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors\n","from pyspark.ml.feature import VectorAssembler\n","\n","dataset = spark.createDataFrame(\n","    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0)],\n","    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])"],"metadata":{"id":"KxNRxswDPx7P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**22. VectorSizeHint**"],"metadata":{"id":"PizpOYm6PyZm"}},{"cell_type":"markdown","source":["กำหนดขนาดของเวกเตอร์ในคอลัมน์ที่มีประเภทข้อมูล Vector เพื่อให้สามารถใช้งานใน Transformer อื่นๆ ที่ต้องการขนาดเวกเตอร์"],"metadata":{"id":"mQqhhEqwP5Hg"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"r5YMeU7va4yQ"}},{"cell_type":"code","source":["from pyspark.ml.linalg import Vectors\n","from pyspark.ml.feature import (VectorSizeHint, VectorAssembler)\n","\n","dataset = spark.createDataFrame(\n","    [(0, 18, 1.0, Vectors.dense([0.0, 10.0, 0.5]), 1.0),\n","     (0, 18, 1.0, Vectors.dense([0.0, 10.0]), 0.0)],\n","    [\"id\", \"hour\", \"mobile\", \"userFeatures\", \"clicked\"])"],"metadata":{"id":"9cz9Er3HP0M7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**23. QuantileDiscretizer**"],"metadata":{"id":"tKta-S-yP1w1"}},{"cell_type":"markdown","source":["แบ่งข้อมูลต่อเนื่องออกเป็นกลุ่มตามควอนไทล์ เช่น การแบ่งข้อมูลออกเป็นหลายช่วงตามจำนวนถัง (buckets)"],"metadata":{"id":"ZFcwKwVCP8O_"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"mT6TQJwVa3kd"}},{"cell_type":"code","source":["from pyspark.ml.feature import QuantileDiscretizer\n","\n","df = spark.createDataFrame([(0, 18.0), (1, 19.0), (2, 8.0), (3, 5.0), (4, 2.2)], [\"id\", \"hour\"])"],"metadata":{"id":"c9o_XxDfP8_1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**24. Imputer**"],"metadata":{"id":"xNC5kPRDP9Wx"}},{"cell_type":"markdown","source":["เติมข้อมูลที่หายไป (missing values) โดยใช้ค่าเฉลี่ย (mean), ค่ามัธยฐาน (median), หรือค่ามากที่สุด (mode) ของข้อมูลที่มีอยู่"],"metadata":{"id":"5XcCqFioQAGc"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"CjiN2gieaxh5"}},{"cell_type":"code","source":["from pyspark.ml.feature import Imputer\n","\n","df = spark.createDataFrame([ (1.0, float(\"nan\")), (2.0, float(\"nan\")), (float(\"nan\"), 3.0), (4.0, 4.0), (5.0, 5.0) ], [\"a\", \"b\"])"],"metadata":{"id":"gmgFGglkQFum"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **Feature Selectors**"],"metadata":{"id":"aqmHQyqfSiET"}},{"cell_type":"markdown","source":["Feature Selectors คือกระบวนการในการเลือกฟีเจอร์ที่สำคัญและมีประโยชน์ที่สุดในการสร้างโมเดลการเรียนรู้ของเครื่อง (Machine Learning) จากชุดข้อมูลที่มีฟีเจอร์หลายตัว เพื่อให้โมเดลสามารถทำงานได้ดีขึ้น โดยการเลือกฟีเจอร์จะช่วยลดความซับซ้อนของโมเดล เพิ่มความเร็วในการฝึกโมเดล และลดโอกาสที่โมเดลจะเกิดการ Overfitting (การที่โมเดลเรียนรู้จากข้อมูลฝึกจนเกินไปจนไม่สามารถทำนายข้อมูลใหม่ได้ดี)"],"metadata":{"id":"Hz5XwCpuY9VH"}},{"cell_type":"markdown","source":["**1. VectorSlicer**"],"metadata":{"id":"jlAZjvFDSn5W"}},{"cell_type":"markdown","source":["VectorSlicer เป็นเครื่องมือที่ใช้ในการแปลงข้อมูล (transformer) ซึ่งจะรับเวกเตอร์ฟีเจอร์และสร้างเวกเตอร์ฟีเจอร์ใหม่ที่มีส่วนย่อยของฟีเจอร์จากเวกเตอร์เดิม ใช้สำหรับการดึงฟีเจอร์จากคอลัมน์เวกเตอร์\n","\n","- การเลือกฟีเจอร์ VectorSlicer รับคอลัมน์เวกเตอร์ที่มีดัชนีที่ระบุ แล้วส่งออกคอลัมน์เวกเตอร์ใหม่ที่มีค่าฟีเจอร์ที่เลือกจากดัชนีเหล่านั้น\n","- ประเภทของดัชนี\n","  - ดัชนีจำนวนเต็ม (Integer indices) ใช้ระบุตำแหน่งในเวกเตอร์ (ใช้ setIndices())\n","  - ดัชนีชื่อฟีเจอร์ (String indices): ใช้ระบุชื่อฟีเจอร์ในเวกเตอร์ (ใช้ setNames()) การใช้ดัชนีนี้จะต้องมี AttributeGroup ในคอลัมน์เวกเตอร์ เพราะการเลือกจะอิงตามชื่อฟีเจอร์\n","- การใช้ดัชนีทั้งสองประเภท สามารถใช้ดัชนีจำนวนเต็มและชื่อฟีเจอร์ร่วมกันได้ แต่จะต้องเลือกฟีเจอร์อย่างน้อยหนึ่งฟีเจอร์ และไม่สามารถมีการซ้ำซ้อนระหว่างดัชนีจำนวนเต็มและชื่อฟีเจอร์ได้\n","- ลำดับของฟีเจอร์ ผลลัพธ์จะเรียงลำดับฟีเจอร์จากดัชนีจำนวนเต็มก่อน (ตามลำดับที่กำหนด) แล้วตามด้วยฟีเจอร์ที่เลือกจากชื่อฟีเจอร์ (ตามลำดับที่กำหนด)\n","โดยรวมแล้ว VectorSlicer ช่วยให้สามารถเลือกและจัดระเบียบฟีเจอร์จากเวกเตอร์ได้ตามความต้องการ"],"metadata":{"id":"BPAVRvrQTlSY"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"WyUyxLtBUpI9"}},{"cell_type":"code","source":["from pyspark.ml.feature import VectorSlicer\n","from pyspark.ml.linalg import Vectors\n","from pyspark.sql.types import Row\n","\n","df = spark.createDataFrame([\n","    Row(userFeatures=Vectors.sparse(3, {0: -2.0, 1: 2.3})),\n","    Row(userFeatures=Vectors.dense([-2.0, 2.3, 0.0]))])"],"metadata":{"id":"5nRFB16aUU58"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**2. RFormula**"],"metadata":{"id":"Q9L679f1W6xz"}},{"cell_type":"markdown","source":["RFormula คือฟีเจอร์ที่ใช้ในการเลือกคอลัมน์ตามสูตรของโมเดล R โดยใช้ตัวดำเนินการบางประเภท เช่น ~, +, -, :, และ . โดยที่\n","\n","`~` ใช้แยกเป้าหมายและตัวแปร\n","\n","`+` รวมตัวแปร เช่น + 0 หมายถึงการลบค่า intercept\n","\n","`-` ลบตัวแปร เช่น - 1 หมายถึงการลบ intercept\n","\n","`:` ใช้สร้างปฏิสัมพันธ์ระหว่างตัวแปร\n","\n","`.` ใช้เลือกคอลัมน์ทั้งหมดยกเว้นตัวแปรเป้าหมาย\n","\n","RFormula จะสร้างคอลัมน์เวกเตอร์ของฟีเจอร์และคอลัมน์ของป้ายกำกับที่เป็นตัวเลขหรือสตริง ซึ่งข้อมูลที่เป็นตัวเลขจะถูกแปลงเป็นตัวเลขทศนิยม ขณะที่ข้อมูลที่เป็นสตริงจะถูกแปลงด้วย StringIndexer และจะทำการ one-hot encoding เมื่อจำเป็น"],"metadata":{"id":"_ra1rLmpW7x1"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"_K8oBb4xbEeO"}},{"cell_type":"code","source":["from pyspark.ml.feature import RFormula\n","\n","df = spark.createDataFrame(\n","    [(7, \"US\", 18, 1.0),\n","     (8, \"CA\", 12, 0.0),\n","     (9, \"NZ\", 15, 0.0)],\n","    [\"id\", \"country\", \"hour\", \"clicked\"])"],"metadata":{"id":"zTdHNYtBX-i0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**3. ChiSqSelector**"],"metadata":{"id":"uYDIllHvajDr"}},{"cell_type":"markdown","source":["ChiSqSelector คือ เครื่องมือในการเลือกฟีเจอร์ที่ใช้การทดสอบ Chi-Squared (Chi²) เพื่อเลือกฟีเจอร์ที่มีความสัมพันธ์สูงกับตัวแปรเป้าหมายในข้อมูลที่มีลักษณะเป็นข้อมูลหมวดหมู่ (categorical data) โดยวิธีการนี้จะเลือกฟีเจอร์ที่มีความสัมพันธ์มากที่สุดระหว่างฟีเจอร์และตัวแปรเป้าหมายผ่านการทดสอบ Chi-Squared ของการเป็นอิสระ (independence test)\n","\n","ChiSqSelector รองรับวิธีการเลือกฟีเจอร์ 5 แบบ ดังนี้\n","\n","1. numTopFeatures เลือกฟีเจอร์ที่ดีที่สุดตามผลการทดสอบ Chi-Squared โดยเลือกจำนวนฟีเจอร์สูงสุดที่กำหนด (ตัวเลือกนี้จะเลือกฟีเจอร์ที่มีความสามารถในการทำนายดีที่สุด).\n","\n","2. percentile คล้ายกับ numTopFeatures แต่จะเลือกฟีเจอร์เป็นสัดส่วนจากฟีเจอร์ทั้งหมด แทนการเลือกจำนวนที่แน่นอน.\n","\n","3. fpr (False Positive Rate) เลือกฟีเจอร์ที่มีค่า p-value ต่ำกว่าค่าระดับที่กำหนด ซึ่งจะช่วยควบคุมอัตราความผิดพลาดที่เกิดจากการเลือกฟีเจอร์ที่ไม่สัมพันธ์จริง (false positives).\n","\n","4. fdr (False Discovery Rate) ใช้กระบวนการ Benjamini-Hochberg ในการเลือกฟีเจอร์ที่มีอัตราการค้นพบผิดพลาด (false discovery rate) ต่ำกว่าค่าที่กำหนด.\n","\n","5. fwe (Family-Wise Error Rate) เลือกฟีเจอร์ที่มีค่า p-value ต่ำกว่าค่าที่กำหนด ซึ่งจะถูกปรับขนาดตามจำนวนฟีเจอร์ทั้งหมด เพื่อควบคุมอัตราความผิดพลาดในระดับครอบครัว (family-wise error rate)\n","\n","โดยปกติแล้ว ChiSqSelector จะใช้วิธี numTopFeatures เป็นค่าเริ่มต้น ซึ่งจะเลือกฟีเจอร์ที่ดีที่สุด 50 ตัว ผู้ใช้สามารถเลือกวิธีการเลือกฟีเจอร์ที่ต้องการได้โดยใช้เมธอด setSelectorType"],"metadata":{"id":"peNMCVyHarKC"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"K6zZLxmdkxgd"}},{"cell_type":"code","source":["from pyspark.ml.feature import ChiSqSelector\n","from pyspark.ml.linalg import Vectors\n","\n","df = spark.createDataFrame([\n","    (7, Vectors.dense([0.0, 0.0, 18.0, 1.0]), 1.0,),\n","    (8, Vectors.dense([0.0, 1.0, 12.0, 0.0]), 0.0,),\n","    (9, Vectors.dense([1.0, 0.0, 15.0, 0.1]), 0.0,)], [\"id\", \"features\", \"clicked\"])\n"],"metadata":{"id":"pRJB7JU1aqxy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**4. UnivariateFeatureSelector**"],"metadata":{"id":"lSueuKOOn6je"}},{"cell_type":"markdown","source":["UnivariateFeatureSelector เป็นเครื่องมือที่ช่วยเลือกฟีเจอร์โดยพิจารณาความสัมพันธ์ระหว่างฟีเจอร์และเป้าหมาย และสามารถเลือกฟีเจอร์ได้โดยใช้หลายวิธีการตามโหมดที่กำหนด เช่น เลือกฟีเจอร์ที่ดีที่สุด, เลือกฟีเจอร์ตามอัตราความผิดพลาด หรือเลือกฟีเจอร์ที่มีค่า p-value ต่ำกว่าค่าที่กำหนด\n","\n","โหมดการเลือกฟีเจอร์ (Selection Modes)\n","\n","1. numTopFeatures เลือกฟีเจอร์จำนวนคงที่ที่ดีที่สุด\n","percentile: เลือกฟีเจอร์ตามสัดส่วนของฟีเจอร์ทั้งหมด (ไม่ใช่จำนวนที่แน่นอน)\n","2. fpr (False Positive Rate) เลือกฟีเจอร์ที่มีค่า p-value ต่ำกว่าค่าที่กำหนด เพื่อควบคุมอัตราการเลือกฟีเจอร์ที่เป็นบวกเท็จ\n","3. fdr (False Discovery Rate) ใช้กระบวนการ Benjamini-Hochberg เพื่อเลือกฟีเจอร์ที่มีอัตราความผิดพลาดต่ำกว่าค่าที่กำหนด\n","4. fwe (Family-Wise Error Rate) เลือกฟีเจอร์ที่มีค่า p-value ต่ำกว่าค่าที่กำหนด ซึ่งค่าของ threshold จะปรับตามจำนวนฟีเจอร์ทั้งหมด เพื่อควบคุมอัตราความผิดพลาดของการเลือกฟีเจอร์แบบครอบครัว"],"metadata":{"id":"5vtDRzd8oTih"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"qnZsUmbno2B1"}},{"cell_type":"code","source":["from pyspark.ml.feature import UnivariateFeatureSelector\n","from pyspark.ml.linalg import Vectors\n","\n","df = spark.createDataFrame([\n","    (1, Vectors.dense([1.7, 4.4, 7.6, 5.8, 9.6, 2.3]), 3.0,),\n","    (2, Vectors.dense([8.8, 7.3, 5.7, 7.3, 2.2, 4.1]), 2.0,),\n","    (3, Vectors.dense([1.2, 9.5, 2.5, 3.1, 8.7, 2.5]), 3.0,),\n","    (4, Vectors.dense([3.7, 9.2, 6.1, 4.1, 7.5, 3.8]), 2.0,),\n","    (5, Vectors.dense([8.9, 5.2, 7.8, 8.3, 5.2, 3.0]), 4.0,),\n","    (6, Vectors.dense([7.9, 8.5, 9.2, 4.0, 9.4, 2.1]), 4.0,)], [\"id\", \"features\", \"label\"])\n","\n","selector = UnivariateFeatureSelector(featuresCol=\"features\", outputCol=\"selectedFeatures\",\n","                                     labelCol=\"label\", selectionMode=\"numTopFeatures\")\n","selector.setFeatureType(\"continuous\").setLabelType(\"categorical\").setSelectionThreshold(1)"],"metadata":{"id":"-2yvLYJoovbk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1738040489166,"user_tz":-420,"elapsed":324,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}},"outputId":"52377fbd-3fc3-4e7a-da7a-5d275b1dc0bc"},"execution_count":72,"outputs":[{"output_type":"execute_result","data":{"text/plain":["UnivariateFeatureSelector_de9c08f3151d"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","source":["result = selector.fit(df).transform(df)\n","result.show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QCo1KrZvST9t","executionInfo":{"status":"ok","timestamp":1738040501951,"user_tz":-420,"elapsed":2227,"user":{"displayName":"083Nattawat Weradechtaweewon","userId":"12849925841226180647"}},"outputId":"f76260a0-da72-41cd-db9a-0d7873cf0ccf"},"execution_count":73,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+-------------------------+-----+----------------+\n","|id |features                 |label|selectedFeatures|\n","+---+-------------------------+-----+----------------+\n","|1  |[1.7,4.4,7.6,5.8,9.6,2.3]|3.0  |[2.3]           |\n","|2  |[8.8,7.3,5.7,7.3,2.2,4.1]|2.0  |[4.1]           |\n","|3  |[1.2,9.5,2.5,3.1,8.7,2.5]|3.0  |[2.5]           |\n","|4  |[3.7,9.2,6.1,4.1,7.5,3.8]|2.0  |[3.8]           |\n","|5  |[8.9,5.2,7.8,8.3,5.2,3.0]|4.0  |[3.0]           |\n","|6  |[7.9,8.5,9.2,4.0,9.4,2.1]|4.0  |[2.1]           |\n","+---+-------------------------+-----+----------------+\n","\n"]}]},{"cell_type":"markdown","source":["**5. VarianceThresholdSelector**"],"metadata":{"id":"Kj979TuGovJO"}},{"cell_type":"markdown","source":["VarianceThresholdSelector เป็นตัวเลือกที่ใช้ลบฟีเจอร์ที่มีความแปรปรวนต่ำ โดยจะลบฟีเจอร์ที่มีความแปรปรวน (variance) น้อยกว่าหรือเท่ากับค่า varianceThreshold ที่กำหนด หากไม่ตั้งค่า varianceThreshold ค่าเริ่มต้นจะเป็น 0 ซึ่งหมายความว่าเฉพาะฟีเจอร์ที่มีค่าแปรปรวนเป็น 0 (คือฟีเจอร์ที่มีค่าเหมือนกันทุกตัวอย่าง) จะถูกลบออก"],"metadata":{"id":"D0QKPcZQpQ9u"}},{"cell_type":"markdown","source":["**Example**"],"metadata":{"id":"ihaCSAXIpmGx"}},{"cell_type":"code","source":["from pyspark.ml.feature import VarianceThresholdSelector\n","from pyspark.ml.linalg import Vectors\n","\n","df = spark.createDataFrame([\n","    (1, Vectors.dense([6.0, 7.0, 0.0, 7.0, 6.0, 0.0])),\n","    (2, Vectors.dense([0.0, 9.0, 6.0, 0.0, 5.0, 9.0])),\n","    (3, Vectors.dense([0.0, 9.0, 3.0, 0.0, 5.0, 5.0])),\n","    (4, Vectors.dense([0.0, 9.0, 8.0, 5.0, 6.0, 4.0])),\n","    (5, Vectors.dense([8.0, 9.0, 6.0, 5.0, 4.0, 4.0])),\n","    (6, Vectors.dense([8.0, 9.0, 6.0, 0.0, 0.0, 0.0]))], [\"id\", \"features\"])"],"metadata":{"id":"mKpT-a7LpXbp"},"execution_count":null,"outputs":[]}]}